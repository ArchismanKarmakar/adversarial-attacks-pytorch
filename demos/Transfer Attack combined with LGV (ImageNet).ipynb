{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Transfer Attack combined with LGV (ImageNet).ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Demo - LGV Transfer Attacks (ImageNet)\n",
    "\n",
    "This notebook demonstrates the LGV technique from the **ECCV22** paper [\"LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity\"](https://arxiv.org/abs/2207.13129) by Martin Gubri, Maxime Cordy, Mike Papadakis, Yves Le Traon from the University of Luxembourg and Koushik Sen from University of California, Berkeley.\n",
    "\n",
    "LGV can be combined with several attacks to improve the transferability of adversarial examples. \n",
    "\n",
    "1. First, LGV collects models along the SGD trajectory with a high learning rate.  The following figure illustrates the approach. In this notebook, we use publically available pretrained models from the original paper. Nevertheless, commented code below is available to collect new models in 10 epochs.\n",
    "\n",
    "![](https://github.com/Framartin/lgv-geometric-transferability/raw/main/lgv/plots/diagram_lr.png?raw=true)\n",
    "\n",
    "2. Second, we apply a standard `torchattacks` attack on one LGV model per iteration. This step does not require any modification of the existing attack, as shown below. Once the models are collected, attacking does not increase the computation cost because we attack a single model per iteration.\n",
    "\n",
    "## 1. Installation"
   ],
   "metadata": {
    "id": "-smvfVCPl43G"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WXv80XDDichl"
   },
   "outputs": [],
   "source": [
    "!pip install torch torchattacks tqdm"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "import glob\n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from torchvision.models import resnet50\n",
    "from torchvision import transforms\n",
    "from torchvision import datasets\n",
    "from torchattacks import LGV, BIM, MIFGSM, DIFGSM, TIFGSM"
   ],
   "metadata": {
    "id": "7K39VJKilo0A"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download LGV models from the original paper. The zip file includes 3 sets of LGV collected models, corresponding to 3 random seeds starting from 3 independently trained DNNs. We will use a single one here, but you may want to report mean and standard deviation computed across several random seeds."
   ],
   "metadata": {
    "id": "tAvgCk-poMO-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!wget -O lgv_models.zip https://figshare.com/ndownloader/files/36698862\n",
    "!unzip lgv_models.zip"
   ],
   "metadata": {
    "id": "y2V2gNoWo5nq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8e374fe8-c81d-4ab9-9dad-63a0b18c4865"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "--2022-08-17 10:59:20--  https://figshare.com/ndownloader/files/36698862\n",
      "Resolving figshare.com (figshare.com)... 52.49.60.170, 52.30.212.171, 2a05:d018:1f4:d003:5e66:753c:3d81:88f7, ...\n",
      "Connecting to figshare.com (figshare.com)|52.49.60.170|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/36698862/models.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20220817/eu-west-1/s3/aws4_request&X-Amz-Date=20220817T105920Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=8b04ff0bbf78d92758e1572519f321b01e935fc7d95784272cf2cd1a9161c1e6 [following]\n",
      "--2022-08-17 10:59:20--  https://s3-eu-west-1.amazonaws.com/pfigshare-u-files/36698862/models.zip?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIYCQYOYV5JSSROOA/20220817/eu-west-1/s3/aws4_request&X-Amz-Date=20220817T105920Z&X-Amz-Expires=10&X-Amz-SignedHeaders=host&X-Amz-Signature=8b04ff0bbf78d92758e1572519f321b01e935fc7d95784272cf2cd1a9161c1e6\n",
      "Resolving s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)... 52.218.84.210\n",
      "Connecting to s3-eu-west-1.amazonaws.com (s3-eu-west-1.amazonaws.com)|52.218.84.210|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 12001708922 (11G) [application/zip]\n",
      "Saving to: ‘lgv_models.zip’\n",
      "\n",
      "lgv_models.zip      100%[===================>]  11.18G  21.4MB/s    in 9m 5s   \n",
      "\n",
      "2022-08-17 11:08:26 (21.0 MB/s) - ‘lgv_models.zip’ saved [12001708922/12001708922]\n",
      "\n",
      "Archive:  lgv_models.zip\n",
      "   creating: models/\n",
      "   creating: models/ImageNet/\n",
      "   creating: models/ImageNet/resnet50/\n",
      "   creating: models/ImageNet/resnet50/cSGD/\n",
      "   creating: models/ImageNet/resnet50/cSGD/seed0/\n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00001.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00002.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00003.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00004.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00005.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00006.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00007.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00008.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00009.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00010.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00011.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00012.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00013.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00014.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00015.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00016.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00017.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00018.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00019.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00020.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00021.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00022.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00023.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00024.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00025.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00026.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00027.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00028.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00029.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00030.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00031.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00032.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00033.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00034.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00035.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00036.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00037.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00038.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00039.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/iter-00040.pt  \n",
      "   creating: models/ImageNet/resnet50/cSGD/seed0/original/\n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed0/original/ImageNet-ResNet50-052e7f78e4db--1564492444-1.pth.tar  \n",
      "   creating: models/ImageNet/resnet50/cSGD/seed1/\n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00001.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00002.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00003.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00004.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00005.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00006.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00007.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00008.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00009.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00010.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00011.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00012.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00013.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00014.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00015.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00016.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00017.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00018.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00019.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00020.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00021.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00022.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00023.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00024.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00025.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00026.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00027.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00028.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00029.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00030.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00031.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00032.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00033.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00034.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00035.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00036.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00037.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00038.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00039.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/iter-00040.pt  \n",
      "   creating: models/ImageNet/resnet50/cSGD/seed1/original/\n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed1/original/ImageNet-ResNet50-1132c260ef75--1564493784-1.pth.tar  \n",
      "   creating: models/ImageNet/resnet50/cSGD/seed2/\n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00001.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00002.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00003.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00004.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00005.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00006.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00007.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00008.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00009.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00010.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00011.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00012.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00013.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00014.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00015.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00016.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00017.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00018.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00019.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00020.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00021.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00022.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00023.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00024.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00025.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00026.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00027.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00028.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00029.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00030.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00031.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00032.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00033.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00034.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00035.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00036.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00037.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00038.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00039.pt  \n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/iter-00040.pt  \n",
      "   creating: models/ImageNet/resnet50/cSGD/seed2/original/\n",
      "  inflating: models/ImageNet/resnet50/cSGD/seed2/original/ImageNet-ResNet50-2f817072e8da--1564493734-1.pth.tar  \n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Set up some constants."
   ],
   "metadata": {
    "id": "oZV-VQaLnvYh"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "PATH_LGV_MODELS=\"models/ImageNet/resnet50/cSGD/seed0\"\n",
    "DATA_PATH=\"/content/drive/MyDrive/data/ILSVRC2012_samples\"\n",
    "BATCH_SIZE_TRAIN=256  # changing batch-size to collect models might require you to tune the LGV learning rate hyperparameter\n",
    "BATCH_SIZE_TEST=64\n",
    "N_WORKERS=5\n",
    "N_EXAMPLES=2000"
   ],
   "metadata": {
    "id": "cwPsBMBolukV"
   },
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load models\n",
    "\n",
    "We load the surrogate and target models. \n"
   ],
   "metadata": {
    "id": "sUamZSc2Mtcr"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def add_normalization_layer(model, mean, std):\n",
    "    \"\"\"\n",
    "    Add a data normalization layer to a model\n",
    "    \"\"\"\n",
    "    return torch.nn.Sequential(\n",
    "        transforms.Normalize(mean=mean, std=std),\n",
    "        model\n",
    "    )"
   ],
   "metadata": {
    "id": "aOHfLJthNQFM"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the LGV collected models."
   ],
   "metadata": {
    "id": "raQ2Osv7Nt2-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# LGV surrogate\n",
    "paths_models = glob.glob(f'{PATH_LGV_MODELS}/*.pt')\n",
    "paths_models = sorted(paths_models)\n",
    "list_models = []\n",
    "for path in paths_models:\n",
    "    model = resnet50()\n",
    "    model.load_state_dict(torch.load(path)['state_dict'])\n",
    "    model = add_normalization_layer(model=model, \n",
    "                                    mean=[0.485, 0.456, 0.406],\n",
    "                                    std=[0.229, 0.224, 0.225])\n",
    "    model = model.eval().cuda()\n",
    "    list_models.append(model)"
   ],
   "metadata": {
    "id": "VM0e7uWYNIQr"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the original DNN from which model collection started."
   ],
   "metadata": {
    "id": "3ITm7WBuNzwi"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "base_model = resnet50()\n",
    "ckpt = torch.load(f'{PATH_LGV_MODELS}/original/ImageNet-ResNet50-052e7f78e4db--1564492444-1.pth.tar')['state_dict']\n",
    "new_state_dict = OrderedDict()\n",
    "for k, v in ckpt.items():\n",
    "    name = k[7:]  # remove `module.`\n",
    "    new_state_dict[name] = v\n",
    "\n",
    "base_model.load_state_dict(new_state_dict)\n",
    "base_model = add_normalization_layer(model=base_model,\n",
    "                                     mean=[0.485, 0.456, 0.406], \n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "base_model = base_model.eval().cuda()\n"
   ],
   "metadata": {
    "id": "fG4i9Sg5NzN4"
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally we load the target model, the pretrained resnet50 model provided by torchvision."
   ],
   "metadata": {
    "id": "dWOITIe6OQiw"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "target_model = resnet50(pretrained=True)\n",
    "target_model = add_normalization_layer(model=target_model, \n",
    "                                       mean=[0.485, 0.456, 0.406], \n",
    "                                       std=[0.229, 0.224, 0.225])\n",
    "target_model = target_model.eval().cuda()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WZInjHVgOZsa",
    "outputId": "7096e976-bde7-424a-839f-9ad82d38ec36"
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n",
      "/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Dataloader\n",
    "\n",
    "Loaders should load unnormalized data (in [0,1]). Here, the test loader is a random subset of 2K test examples. "
   ],
   "metadata": {
    "id": "2kxpmNbnOpgQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "traindir = os.path.join(DATA_PATH, 'train')\n",
    "transform_train = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(224),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "trainset = datasets.ImageFolder(traindir, transform_train)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=BATCH_SIZE_TRAIN,\n",
    "                                          shuffle=True, num_workers=N_WORKERS,\n",
    "                                          pin_memory=True)\n",
    "testdir = os.path.join(DATA_PATH, 'validation')\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "testset = datasets.ImageFolder(testdir, transform_test)\n",
    "indices = torch.from_numpy(np.random.choice(len(testset), size=(N_EXAMPLES,),\n",
    "                                            replace=False))\n",
    "# select a subset of 2K examples. The original paper selects only original \n",
    "# examples that are correctly predicted by the target model, which might explain\n",
    "# why we obtain slightly different results.\n",
    "\n",
    "testsubset = torch.utils.data.Subset(trainset, indices)\n",
    "testloader = torch.utils.data.DataLoader(testsubset, batch_size=BATCH_SIZE_TEST,\n",
    "                                         shuffle=False, num_workers=N_WORKERS,\n",
    "                                         pin_memory=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mcmyJbq1OvO5",
    "outputId": "afa3d749-d90c-4636-ff31-cf97c647d9b9"
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 5 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  cpuset_checked))\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Attacks\n",
    "\n",
    "We can easily combine LGV with attacks available out-of-the-box in `torchattacks`. Then, we report the success rates of the vanilla attacks."
   ],
   "metadata": {
    "id": "zoiI9zBsO-sp"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def report_success_rate(atk):\n",
    "    \"\"\"\n",
    "    Compute the success rate of the provided attack on test images\n",
    "    \"\"\"\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in testloader:\n",
    "        images = images.cuda()\n",
    "        with torch.no_grad():\n",
    "            outputs = target_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "        adv_images = atk(images, predicted)\n",
    "        with torch.no_grad():\n",
    "            outputs_adv = target_model(adv_images)\n",
    "            _, predicted_adv = torch.max(outputs_adv.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted_adv == predicted).sum()\n",
    "    print(f\"Success rate of {type(atk).__name__}{'-'+atk.attack_class.__name__ if hasattr(atk, 'attack_class') else ''}: {100 - 100 * float(correct) / total}%\")\n"
   ],
   "metadata": {
    "id": "OaX72F8fPjm6"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LGV + BIM\n",
    "\n",
    "The most classical is to use LGV on top of BIM. \n",
    "\n",
    "- Below we provide commented code to collect the models yourself if you would rather not use the pretrained collected models. Collecting new models leads to similar results.\n",
    "- Ideally, you want the number of attack iterations to be greater or equal than the number of collected models (`epochs * nb_models_epoch`). Otherwise, consider setting `full_grad=True`. As shown in the paper, 50 BIM iterations have higher success rate than 10 BIM iterations.\n",
    "- Changing the surrogate architecture or the training batch-size may require you to tune the learning rate (`lr`) hyperparameter. Preliminary experiments suggested that `lr=0.1` might be slightly better than the default 0.05 for some architectures.\n",
    "- If you are limited in computation cost or memory, you can reduce the number of epochs to 5 and the number of models per epoch to 2, without impacting too much the success rates."
   ],
   "metadata": {
    "id": "-GwSJI-OP259"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "atk = LGV(base_model, trainloader, lr=0.05, epochs=10, nb_models_epoch=4, \n",
    "          wd=1e-4, full_grad=False, attack_class=BIM, eps=4/255, alpha=4/255/10,\n",
    "          steps=50, verbose=True)\n",
    "atk.load_models(list_models)  # load our list of collected models\n",
    "\n",
    "# uncomment the next 2 lines and comment the last one to collect models yourself (10 ImageNet epochs)\n",
    "#atk.collect_models()\n",
    "#atk.save_models('models/lgv')\n",
    "\n",
    "report_success_rate(atk)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p8750gjBP0lU",
    "outputId": "14c9ca50-66a0-45bc-9ab3-701c307b6640"
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Phase 2: craft adversarial examples with BIM\n",
      "Success rate of LGV-BIM: 98.6%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## LGV with other attacks\n",
    "\n",
    "LGV can be easily combined with other attacks provided by `torchattacks`. By default, LGV compute the gradient of a single model at each iteration (`full_grad=False`). The parameter `full_grad` should be set to `True` for single-step attacks, such as `FGSM`, to compute the gradient against all available models."
   ],
   "metadata": {
    "id": "dHvCPWNdRTJ0"
   }
  },
  {
   "cell_type": "raw",
   "source": [
    "If you have collected models yourself (using `LGV.collect_models()`), you may want to load the saved models to the new `LGV` instance as below:"
   ],
   "metadata": {
    "id": "v-2fw-REez10",
    "pycharm": {
     "name": "#%% raw\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "#paths_models = glob.glob('models/lgv/*.pt')\n",
    "#paths_models.sort()\n",
    "#list_models = []\n",
    "#for path in paths_models:\n",
    "#  model = resnet50()\n",
    "#  model.load_state_dict(torch.load(path)['state_dict'])\n",
    "#  model = add_normalization_layer(model=model, mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "#  model = model.eval().cuda()\n",
    "#  list_models.append(model)"
   ],
   "metadata": {
    "id": "h4m29Pg-ezIA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "We evaluate LGV when combined with more advanced attacks than BIM."
   ],
   "metadata": {
    "id": "7tFeND9hfsaW"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# LGV+MI\n",
    "atk = LGV(base_model, trainloader, lr=0.05, epochs=10, nb_models_epoch=4, wd=1e-4, full_grad=False, attack_class=MIFGSM, eps=4/255, alpha=4/255/10, steps=50, verbose=True)\n",
    "atk.load_models(list_models)\n",
    "report_success_rate(atk)\n",
    "\n",
    "# LGV+DI\n",
    "atk = LGV(base_model, trainloader, lr=0.05, epochs=10, nb_models_epoch=4, wd=1e-4, full_grad=False, attack_class=DIFGSM, eps=4/255, alpha=4/255/10, steps=50, verbose=True)\n",
    "atk.load_models(list_models)\n",
    "report_success_rate(atk)\n",
    "\n",
    "# LGV+DI+MI\n",
    "atk = LGV(base_model, trainloader, lr=0.05, epochs=10, nb_models_epoch=4, wd=1e-4, full_grad=False, attack_class=DIFGSM, eps=4/255, alpha=4/255/10, steps=50, decay=1.0, verbose=True)\n",
    "atk.load_models(list_models)\n",
    "report_success_rate(atk)\n",
    "\n",
    "# LGV+TI\n",
    "atk = LGV(base_model, trainloader, lr=0.05, epochs=10, nb_models_epoch=4, wd=1e-4, full_grad=False, attack_class=TIFGSM, eps=4/255, alpha=4/255/10, steps=50, verbose=True)\n",
    "atk.load_models(list_models)\n",
    "report_success_rate(atk)\n",
    "\n",
    "# LGV+TI+MI\n",
    "atk = LGV(base_model, trainloader, lr=0.05, epochs=10, nb_models_epoch=4, wd=1e-4, full_grad=False, attack_class=TIFGSM, eps=4/255, alpha=4/255/10, steps=50, decay=1.0, verbose=True)\n",
    "atk.load_models(list_models)\n",
    "report_success_rate(atk)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hrDQTlblRcEk",
    "outputId": "6a0d0dfe-54ff-4319-ba10-888f4d697b62"
   },
   "execution_count": 14,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Phase 2: craft adversarial examples with MIFGSM\n",
      "Success rate of LGV-MIFGSM: 98.7%\n",
      "Phase 2: craft adversarial examples with DIFGSM\n",
      "Success rate of LGV-DIFGSM: 97.0%\n",
      "Phase 2: craft adversarial examples with DIFGSM\n",
      "Success rate of LGV-DIFGSM: 98.55%\n",
      "Phase 2: craft adversarial examples with TIFGSM\n",
      "Success rate of LGV-TIFGSM: 78.45%\n",
      "Phase 2: craft adversarial examples with TIFGSM\n",
      "Success rate of LGV-TIFGSM: 83.55%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Baselines\n",
    "\n",
    "We can now compare with the vanilla attacks, i.e., without LGV. A simple LGV+BIM attack beats all other attacks by a large margin. "
   ],
   "metadata": {
    "id": "J6bZcPCORfrB"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# BIM\n",
    "atk = BIM(base_model, eps=4/255, alpha=4/255/10, steps=50)\n",
    "report_success_rate(atk)\n",
    "\n",
    "# MI\n",
    "atk = MIFGSM(base_model, eps=4/255, alpha=4/255/10, steps=50)\n",
    "report_success_rate(atk)\n",
    "\n",
    "# DI\n",
    "atk = DIFGSM(base_model, eps=4/255, alpha=4/255/10, steps=50)\n",
    "report_success_rate(atk)\n",
    "\n",
    "# DI+MI\n",
    "atk = DIFGSM(base_model, eps=4/255, alpha=4/255/10, steps=50, decay=1.0)\n",
    "report_success_rate(atk)\n",
    "\n",
    "# TI\n",
    "atk = TIFGSM(base_model, eps=4/255, alpha=4/255/10, steps=50)\n",
    "report_success_rate(atk)\n",
    "\n",
    "# TI+MI\n",
    "atk = TIFGSM(base_model, eps=4/255, alpha=4/255/10, steps=50, decay=1.0)\n",
    "report_success_rate(atk)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RD8S_aYRtZm",
    "outputId": "62a00770-940d-443a-cd17-61a52ff60ca7"
   },
   "execution_count": 15,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Success rate of BIM: 71.65%\n",
      "Success rate of MIFGSM: 78.7%\n",
      "Success rate of DIFGSM: 89.7%\n",
      "Success rate of DIFGSM: 93.05%\n",
      "Success rate of TIFGSM: 70.35%\n",
      "Success rate of TIFGSM: 75.25%\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Credits\n",
    "\n",
    "This notebook was written by Martin Gubri. If you use this code in your research, please consider citing the LGV paper:\n",
    "\n",
    "```\n",
    "@inproceedings{,\n",
    "   abstract = {We propose transferability from Large Geometric Vicinity (LGV), a new technique to increase the transferability of black-box ad-versarial attacks. LGV starts from a pretrained surrogate model and collects multiple weight sets from a few additional training epochs with a constant and high learning rate. LGV exploits two geometric properties that we relate to transferability. First, models that belong to a wider weight optimum are better surrogates. Second, we identify a subspace able to generate an effective surrogate ensemble among this wider optimum. Through extensive experiments, we show that LGV alone outper-forms all (combinations of) four established test-time transformations by 1.8 to 59.9 percentage points. Our findings shed new light on the importance of the geometry of the weight space to explain the transferability of adversarial examples.},\n",
    "   author = {Martin Gubri and Maxime Cordy and Mike Papadakis and Yves Le Traon and Koushik Sen},\n",
    "   keywords = {Adversarial Examples,Deep Learning,Loss Geometry,Machine Learning Security,Transferability},\n",
    "   publisher = {ECCV 2022},\n",
    "   title = {LGV: Boosting Adversarial Example Transferability from Large Geometric Vicinity},\n",
    "   url = {https://github.com/Framartin/lgv-geometric-transferability},\n",
    "}\n",
    "```"
   ],
   "metadata": {
    "id": "ksHNlkMIK8Nb"
   }
  }
 ]
}